# k-Nearest Neighbours

# k-Nearest Neighbours I

## History

k-NN is the simplest of all machine learning algorithms

The philosophy behind k-NN dates back very early (965-1040) by a scientist in optics and perception

## k-NN Classification

<aside>
üôÄ Recall what classification is:
Classification is when the output is a category

</aside>

Constructing a model that can classify a given point to one of several option classes (categorise them)

Example data: $(\{x_i,y_i \}^{n})_{i_1}$

Feature vector: $x_i \in R^d$

Class label: $y_i \in \{1,2,...,c\}$

### Iris Classification

Iris data: https://archive.ics.uci.edu/mldatasets/iris

Dataset contains 150 iris samples from 3 classes that are:

1. Setosa
2. Versicolour
3. Virginica

Each class is characterised by it‚Äôs sepal length and petal width measured

Task: Predict the flower type of a query iris sample

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled.png)

### How does k-NN classification here work?

Given a set of training samples {features, class label}, each sample corresponds to a data point in the feature space.

k-NN classification rule:

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%201.png)

We are given a new sample $x_{te}$ (testing point) and we do not know what class it is from. What we do next is calculate the distance between the testing point and other training data points ($x_{tr}$). After doing so, sort the distances and find k training samples that are closer to your testing sample. After doing so, check the common class near to your testing point. Check the picture on the right.

This idea is also called majority voting (assigning the class that is common)

![As we can see, inside the red-dotted circle, we have 2 ‚Äò+‚Äô and 1 ‚Äò-‚Äô, it is safe for us to assume that the testing point belongs to the ‚Äò+‚Äô class as that is the most common class here](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%202.png)

As we can see, inside the red-dotted circle, we have 2 ‚Äò+‚Äô and 1 ‚Äò-‚Äô, it is safe for us to assume that the testing point belongs to the ‚Äò+‚Äô class as that is the most common class here

## 1-NN for Binary Classification

We have 9 training samples, 6 from red class and 3 from blue class. We need to figure out what class are these other samples from (the triangle ones)

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%203.png)

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%204.png)

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%205.png)

Simply calculate the distances to the 9 training samples and sort the distances and find the nearest neighbour (k = 1), and then assign that class, and do the same for other samples too.

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%206.png)

## k-NN Regression

<aside>
üôÄ Recall what regression was
The output to be continuous numbers

</aside>

Example data: $(\{x_i,y_i \}^{n})_{i_1}$, here $x_i$ is input and $y_i$ is output and both are continuous numbers

Feature vector: $x_i \in R^d$

Output $y_i \in R^k$

### k-NN Regression Rule

Given a set of training samples {features, output}

Each sample corresponds to a data point in the feature space

k-NN regression rule:

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%207.png)

Has a very similar approach to the k-NN classification rule, you measure the distance between the testing point and the training samples, sort the distances and select k nearest points. However you do not go with the majority voting, you calculate $y_{te}$ by averaging the output of those nearest points: 

$$
y_{te} = (y_{NN1}+y_{NN2}+...+y_{NNk})/k
$$

<aside>
üôÄ Classification ‚Üí majority voting
Regression ‚Üí Averaging the output of nearest points

</aside>

### Regression Example 1

Training samples = 200 input-output pairs (blue circles)

Predict the single-output y from the single-input x

‚ÄúEach training sample is generated by first choosing a random input x between -20 and 20, and then calculating the output by y=sin(x).‚Äù

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%208.png)

Suppose we have a testing point at x = 6.0, what would y be equals to?

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%209.png)

To calculate y, assuming this is a 3-NN regression model, search the three nearest neighbours to the x-value of the testing sample which was 6.0. Find the output values of the 3 neighbours

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2010.png)

Then, average the three values

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2011.png)

Can the model be reliable for predicting output for input values outside [-20,20]?

Ans) No. Because the model is only trained for values inside that range.

### Regression Example 2 (Image Completion Problem)

Data: ORL Database of faces 

The dataset contains face images of 40 people, with 10 images per person

Each image contains 32 x 32 = 1024 pixels, with 256 grey levels per pixel

## Instance-based Learning

Many algorithms are developed to predict output based on the similarity (or distance) of the query to it‚Äôs nearest neighbour(s) in the training set.

Representative algorithm: k-NN

Aspects to be considered: 

- How to compute the distance
- How to choose number of neighbours (k)
- How to infer the output from neighbouring points

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2012.png)

Task: Guess the right face from the left face (image completion)

- Input variables (x): the 512 pixels of the left side of an image (given)
- Output variables (y): the other 512 pixels of the image (to be estimated)

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2013.png)

Build a 3-NN regression model using 200 training images, containing 5 example images (with both left and right faces) for each of the 40 people.

Compare Euclidean distances using image pixels, and search the 3 most similar left faces 

<aside>
üôÄ Euclidean distance is defined as **the distance between two points**

</aside>

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2014.png)

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2015.png)

More testing images:

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2016.png)

---

# k-Nearest Neighbour II

## Distance Calculation (Measure)

An important quantity to compute in a k-NN model is distance

The most commonly used distance is **Euclidean Distance:**

Given two d-dimensional data points (d):

$p = [p_1, p_2, ..., p_d], q= [q_1, q_2, ..., q_d]$

$d(p,q) = \sqrt{(p_1-q_1)^2+(p_2-q_2)^2+...+(p_d-q_d)^2}$

$= \sqrt{\sum_{i=1}^{d}(p_i-q_i)^2}$ = $|| p-q||_2$ 

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2017.png)

Minkowski Distance:

$$
d(p,q) = [|p_1-q_1|^t + |p_2 - q_2|^t + ... + |p_d-q_d|^t ]^{1/t} = [\sum_{i=1}^d|p_i-q_i|^t]^{t/2}
$$

$t =2 :$ Euclidean Distance

$t = 1:$  Manhattan (City Block) Distance

Minkowski distance is a more general way to calculate the distance

![Note that city block is base + perpendicular, whereas Euclidean distance is the hypotenuse in this case (wrong, you do not square base and perp for city block)](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2018.png)

Note that city block is base + perpendicular, whereas Euclidean distance is the hypotenuse in this case (wrong, you do not square base and perp for city block)

### Similarity Measure

The k nearest neighbours have the highest similarity values

Inner product and cosine similarity are good similarity measures for capturing co-occurrence pattern:
$s_{inner}(p,q) = \sum_{i = 1}^{d}p_iq_i = p^Tq,$

$$
s_{cos}(p,q) =\frac {\sum_{i=1}^dp_iq_i}{\sqrt{\sum_{i=1}^dp_i^2}\sqrt{\sum_{i=1}^dq_i^2}} = \frac{p^Tq}{||p||_2||q||_2}
$$

- From the slides
    
    ![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2019.png)
    

Convert cosine to distance:

$d(p,q) = 1 - s_{cos}(p,q)$

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2020.png)

## Effect of training samples

- Small number of training samples:
    - insufficient information
- Large number of training samples:
    - more information
    - but time, memory cost consumption (distance calculation, sorting)
- Noisy training samples:
    - inaccurate prediction
- Example - A binary 1-NN classifier trained by 9 iris samples
    
    ![Imagine to apply the 1-NN rule to every single point in the space. The whole space will be divided into red and blue areas.. The left figure displays such effect.](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2021.png)
    
    Imagine to apply the 1-NN rule to every single point in the space. The whole space will be divided into red and blue areas.. The left figure displays such effect.
    
    ![This is equivalent to using the nonlinear separation boundary (highlighted by the black line) to partition the data space to two classes](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2022.png)
    
    This is equivalent to using the nonlinear separation boundary (highlighted by the black line) to partition the data space to two classes
    
    ![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2023.png)
    

![We can then see it has incorrectly classified some samples, and we can then calculate error rate and accuracy](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2024.png)

We can then see it has incorrectly classified some samples, and we can then calculate error rate and accuracy

Error rate = $\frac {incorrectly \space classified}{total \space samples}$= $\frac {9}{60} = 15$%

Accuracy = $\frac {correctly \space classified} {total samples}$= $\frac {51}{60} = 85$%

The 9 training samples do not summarise well the data distribution in each class.

Insufficient information

### What happens if we increase the number of training samples?

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2025.png)

As observed, increasing the number of training samples, we get a higher rate of accuracy.

Similarly:

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2026.png)

### Effect of Noisy Training Samples

![Region conflict?](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2027.png)

Region conflict?

## Effect of Neighbour Number k

- Hyper-parameter: neighbour number k
- The process of determining the neighbour number k is called hyper-parameter selection or model selection
- it is **not** a good idea to set k as **even** number for binary classification

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2028.png)

### Types of k:

1. Small k: 
    
    We may model noise
    
2. Large k
    
    Neighbours will include too many samples from other classes, which can negatively affect the prediction 
    

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2029.png)

Neighbour number k will affect how you partition the space

Also when we have wrong training samples, the model can predict incorrect representation of samples

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2030.png)

Increasing k-value, you make your prediction based on more points, which can improve your accuracy

In this case, we can see that k = 1 case is more sensitive to those noisy samples than the k = 11 case

### Too large k: 3-class k-NN

![Untitled](k-Nearest%20Neighbours%20fa9f0acebec74eb2a1f56c6909afb9c4/Untitled%2031.png)

Many samples from other classes are identified as neighbours. The decision can become unreliable

In the image completion problem too, too high neighbour number includes bad images (wrong faces) in estimation, resulting in bad results.

## Neighbour Search Algorithm

A na√Øve approach to search neighbours is brute-forcing compute distances between all pairs of samples and then sort. 

Brute forcing is never efficient and can take up too much computation operations. There could be efficient operations for example, tree based methods:

Basic idea: knowing A is very distant from B, and B is very close to C, it is certain that A and C are very distant, without having to explicitly calculate their distance. Apply this to reduce the number of calculations.

Varieties: K-D Tree, Ball tree, e.t.c.

[1.6. Nearest Neighbors](https://scikit-learn.org/stable/modules/neighbors.html)

# Summary

- k-NN is the simplest machine learning algorithm
- It can be used for both classification and regression
- No **explicit** training
    - A non-parametric method: no parameter to be optimised
- The algorithm relies on a distance measure
- More training data provides more information to learn, but results in high memory cost (needs to store all the training data)
- The neighbour number k is a hyper-parameter to be selected by the user:
    - Too small: sensitive to noise
    - Too large: Inaccurate prediction